#!/usr/bin/env python
"""
Newsgroup post classification

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging
import argparse

from pathlib import Path
import tensorflow as tf
import numpy as np
import keras

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import LSTM
from project.load_text_data import load_text_from_files
from project.text_to_index import map_label_to_index
from project.text_to_index import map_text_list_to_word_list
from project.text_to_index import map_text_to_token_matrix
from project.text_to_index import map_word_list_to_vocabulary

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCH_SIZE = 20
TOP_VOCABULARY_SIZE = 10000

# Example of data dir:
# ../../../../ai/dataset/20/20newsgroups_fromto_only/20news-18828
# Specify this with --data_dir option on the command line

def example(data_dir=None):
    """Train the model and predict.

    Parameters
    ----------
    data_dir: str
        Directory where the data is stored.

    raises
    -------
    ValueError
        If data_dir is not set.
    """
    if data_dir is None:
        raise ValueError("data_dir is not set.")

    # Load text data
    (x_train_text, y_train_labels), (x_test_text, y_test_labels) = \
        load_text_from_files(Path(data_dir), test_dataset_ratio=0.2, errors='ignore')

    label_to_index, index_to_label = map_label_to_index(y_train_labels)

    num_files_train = len(x_train_text)
    num_labels_train = len(set(y_train_labels))

    num_files_test = len(x_test_text)
    num_labels_test = len(set(y_test_labels))

    log.info("Number of labels found in training dataset: %d" % (num_labels_train))
    log.info("Number of posts found in training dataset: %d" % (num_files_train))

    log.info("Number of labels found in test dataset: %d" % (num_labels_test))
    log.info("Number of posts found in test dataset: %d" % (num_files_test))

    assert num_labels_train == num_labels_test

    # Break up entire text to words
    all_words = map_text_list_to_word_list(x_train_text)

    vocab, vocabulary_size, top_vocabulary, top_vocabulary_size, reserved_word_size, word_to_index, index_to_word = \
        map_word_list_to_vocabulary(all_words, TOP_VOCABULARY_SIZE)

    # Convert train text to tokens
    x_train, y_train = map_text_to_token_matrix(x_train_text, y_train_labels,
                                          top_vocabulary_size, reserved_word_size, num_labels_train, label_to_index, word_to_index)

    x_test, y_test = map_text_to_token_matrix(x_test_text, y_test_labels,
                                          top_vocabulary_size, reserved_word_size, num_labels_test, label_to_index,
                                          word_to_index)

    # Set up a model
    model = Sequential()
    # model.add(LSTM(512, input_shape=x_train.shape[1:], return_sequences=False)) # input_shape.  Change (60000, 28, 28) to (28, 28)
    model.add(Dense(128, activation='relu', input_shape=(top_vocabulary_size+reserved_word_size,)))
    model.add(Dense(20, activation='softmax'))
    # Note the use of tf.train.AdamOptimizer instead of tf.keras.optimizers.Adam
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                   optimizer=optimizer, metrics=['accuracy'])

    model.fit(x=x_train, y=y_train, epochs=EPOCH_SIZE)

    # Test against test dataset
    y_hat_test_one_hot = model.predict(x_test)

    total_size = y_hat_test_one_hot.shape[0]
    y_hat_test_one_hot_int = np.argmax(y_hat_test_one_hot, axis=1)  # to int from one-hot vector
    y_test_int = np.argmax(y_test, axis=1)  # to int from one-hot vector

    matched_indices = (y_hat_test_one_hot_int == y_test_int)
    matched_count = y_test[matched_indices].shape[0]
    log.info(
         "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""

    parser = argparse.ArgumentParser(description='Newsgroup post classifier')
    parser.add_argument('--data_dir',
                        type=str,
                        help="Data directory")

    args = parser.parse_args()
    data_dir = args.data_dir

    example(data_dir = data_dir)


if __name__ == "__main__":
    main()
