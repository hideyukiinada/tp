#!/usr/bin/env python
"""
Newsgroup post classification

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging

from pathlib import Path
import tensorflow as tf
import numpy as np
import keras

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import LSTM
from project.load_text_data import load_text_from_files
from project.load_text_data import label_to_index

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCH_SIZE = 20
TOP_VOCABULARY_SIZE = 999

DATA_DIR = "../../../../ai/dataset/20/20newsgroups_fromto_only/20news-18828"  # FIXME


def example():
    """Train the model and predict.
    """

    (x_train, y_train), (x_test, y_test) = load_text_from_files(Path(DATA_DIR), test_dataset_ratio=0.2, errors='ignore')

    label2index, index2label = label_to_index(y_train)


    num_files_train = len(x_train)
    num_labels_train = len(set(y_train))

    num_files_test = len(x_test)
    num_labels_test = len(set(y_test))

    log.info("Number of labels found in training dataset: %d" % (num_labels_train))
    log.info("Number of posts found in training dataset: %d" % (num_files_train))

    log.info("Number of labels found in test dataset: %d" % (num_labels_test))
    log.info("Number of posts found in test dataset: %d" % (num_files_test))

    assert num_labels_train == num_labels_test

    all_words = list()
    for text_in_each_file in x_train:
        filters = '\'!"#$%&()*+,.<>-/;=?@[\]^_`{|}~'
        text_in_each_file = str(text_in_each_file)

        text_in_each_file = text_in_each_file.replace("\\n", " ")

        words_in_text = keras.preprocessing.text.text_to_word_sequence(text_in_each_file,
                                                                       filters=filters, lower=False, split=' ')
        all_words += words_in_text

    # Count occurrence
    word_count = dict()
    for w in all_words:
        if w not in word_count:
            word_count[w] = 0

        count = word_count[w]
        word_count[w] = count + 1

    # The unique words in across all the files
    vocab = sorted(word_count.keys(), key=lambda v: word_count[v], reverse=True)
    vocabulary_size = len(vocab)
    log.info("Size of vocabulary: %d" % (vocabulary_size))
    top_vocabulary_size = min(vocabulary_size, TOP_VOCABULARY_SIZE)
    log.info("Size of top vocabulary: %d" % (top_vocabulary_size))

    top_vocabulary = vocab[:top_vocabulary_size]

    # print top 10
    log.info("Top 5 words (count)")
    for i in range(5):
        log.info("%10s (%d)" % (top_vocabulary[i], word_count[top_vocabulary[i]]))

    # Mapping between words and IDs
    word_to_id = {w: i + 1 for i, w in enumerate(top_vocabulary)}  # +1 for unknown
    id_to_word = {i: w for w, i in word_to_id.items()}

    keys = word_to_id.keys()
    for i, k in enumerate(keys):
        log.info("%d for %s" % (word_to_id[k], k))
        if i + 1 == 5:
            break

    keys = id_to_word.keys()
    for i, k in enumerate(keys):
        log.info("%s for %d" % (id_to_word[k], k))
        if i + 1 == 5:
            break

    x_list = list()
    y_list = list()

    # Convert text to tokens
    for i, text in enumerate(x_train):
        log.debug("Processing post: [%d]" % (i+1))

        text = text.replace("\\n", " ") # FIXME. Merge

        filters = '\'!"#$%&()*+,-/;=?@[\]^_`{|}~'
        words_in_text = keras.preprocessing.text.text_to_word_sequence(text,
                                                                       filters=filters, lower=False, split=' ')
        word_id_list = list()
        for w in words_in_text:
            if w not in word_to_id:
                id = 0  # Unknown
            else:
                id = word_to_id[w]
            word_id_list.append(id)

        word_array = np.array(word_id_list)
        #log.info(word_array.shape)

        word_array_one_hot = keras.utils.to_categorical(word_array, top_vocabulary_size+1).astype(np.float32)
        #log.info(word_array_one_hot.shape)

        s = np.sum(word_array_one_hot, axis=0)
        #log.info(s.shape)
        s = s.reshape(1, 1000)

        x_list.append(s)

        # For now, do not change non-zero element to 1.
        label_index = label2index[y_train[i]]
        label_index = keras.utils.to_categorical(label_index, 20).astype(np.float32)
        label_index = label_index.reshape(1, 20)
        y_list.append(label_index)

        # change to one-hot

    x_train = np.concatenate(x_list, axis=0)
    print(x_train.shape)
    y_train = np.concatenate(y_list)

    # Set up a model
    model = Sequential()
    # model.add(LSTM(512, input_shape=x_train.shape[1:], return_sequences=False)) # input_shape.  Change (60000, 28, 28) to (28, 28)
    model.add(Dense(128, activation='relu', input_shape=(1000,)))
    model.add(Dense(20, activation='softmax'))
    # Note the use of tf.train.AdamOptimizer instead of tf.keras.optimizers.Adam
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                   optimizer=optimizer, metrics=['accuracy'])

    model.fit(x=x_train, y=y_train, epochs=EPOCH_SIZE)

    #y_hat_test_one_hot = model.predict(x_test)

    # total_size = y_hat_test_one_hot.shape[0]
    # y_hat_test_one_hot_int = np.argmax(y_hat_test_one_hot, axis=1)  # to int from one-hot vector
    #
    # matched_indices = (y_hat_test_one_hot_int == y_test)
    # matched_count = y_test[matched_indices].shape[0]
    # log.info(
    #     "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""
    example()


if __name__ == "__main__":
    main()
