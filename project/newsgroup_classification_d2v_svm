#!/usr/bin/env python
"""
Newsgroup post classification using doc2vec

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging
import argparse

from pathlib import Path
import numpy as np


from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.linear_model import SGDClassifier
from sklearn import metrics

from project.load_text_data import load_text_from_files
from project.text_to_embedding_doc2vec import map_text_list_to_embedding
from project.text_to_id import map_label_to_id

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCH_SIZE = 40
TOP_VOCABULARY_SIZE = 10000
NUM_WORDS_PER_DOC = 512


# Example of data dir:
# ../../../../ai/dataset/20/20newsgroups_fromto_only/20news-18828
# Specify this with --data_dir option on the command line

def example(data_dir=None):
    """Train the model and predict.

    Parameters
    ----------
    data_dir: str
        Directory where the data is stored.

    raises
    -------
    ValueError
        If data_dir is not set.
    """
    if data_dir is None:
        raise ValueError("data_dir is not set.")

    # Load text data
    (x_train_text, y_train_labels), (x_test_text, y_test_labels) = \
        load_text_from_files(Path(data_dir), test_dataset_ratio=0.2, errors='ignore')

    label_to_id, id_to_label = map_label_to_id(y_train_labels)

    num_files_train = len(x_train_text)
    num_labels_train = len(set(y_train_labels))

    num_files_test = len(x_test_text)
    num_labels_test = len(set(y_test_labels))

    log.info("Number of labels found in training dataset: %d" % (num_labels_train))
    log.info("Number of posts found in training dataset: %d" % (num_files_train))

    log.info("Number of labels found in test dataset: %d" % (num_labels_test))
    log.info("Number of posts found in test dataset: %d" % (num_files_test))

    assert num_labels_train == num_labels_test

    # Break up entire text to words
    x_train_v, y_train = map_text_list_to_embedding(x_train_text, y_train_labels,
                                                    num_labels_train,
                                                    label_to_id)
    print("x_train_v.shape")
    print(x_train_v.shape)
    print("y_train.shape")
    print(y_train.shape)

    x_test_v, y_test = map_text_list_to_embedding(x_test_text, y_test_labels,
                                                                 num_labels_test,
                                                                 label_to_id)

    y_train_int = np.argmax(y_train, axis=1)
    print("y_train_int.shape")
    print(y_train_int.shape)

    svm = SGDClassifier(loss='hinge', penalty='l2',
                        alpha=1e-3, random_state=42,
                        max_iter=5, tol=None)

    svm.fit(x_train_v, y_train_int)

    y_hat_test = svm.predict(x_test_v)
    print("y_hat_test.shape")
    print(y_hat_test.shape)

    # Test against test dataset
    total_size = y_hat_test.shape[0]
    y_test_int = np.argmax(np.array(y_test), axis=1)
    print("y_test_int.shape")
    print(y_test_int.shape)

    matched_indices = (y_hat_test == y_test_int)

    matched_array = y_test_int[matched_indices]
    matched_count = matched_array.shape[0]
    log.info(
        "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""

    parser = argparse.ArgumentParser(description='Newsgroup post classifier')
    parser.add_argument('--data_dir',
                        type=str,
                        help="Data directory")

    args = parser.parse_args()
    data_dir = args.data_dir

    example(data_dir=data_dir)


if __name__ == "__main__":
    main()
