#!/usr/bin/env python
"""
Newsgroup post classification using embedding

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging
import argparse

from pathlib import Path
import tensorflow as tf
import numpy as np
import keras

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import Embedding
from tensorflow.python.keras.layers import GlobalAveragePooling1D

from project.load_text_data import load_text_from_files
from project.text_to_id import map_label_to_id
from project.text_to_id import map_text_list_to_word_list
from project.text_to_id import map_text_to_word_id
from project.text_to_id import map_word_list_to_vocabulary

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCH_SIZE = 20
TOP_VOCABULARY_SIZE = 10000
NUM_WORDS_PER_DOC = 512


# Example of data dir:
# ../../../../ai/dataset/20/20newsgroups_fromto_only/20news-18828
# Specify this with --data_dir option on the command line

def example(data_dir=None):
    """Train the model and predict.

    Parameters
    ----------
    data_dir: str
        Directory where the data is stored.

    raises
    -------
    ValueError
        If data_dir is not set.
    """
    if data_dir is None:
        raise ValueError("data_dir is not set.")

    # Load text data
    (x_train_text, y_train_labels), (x_test_text, y_test_labels) = \
        load_text_from_files(Path(data_dir), test_dataset_ratio=0.2, errors='ignore')

    label_to_id, id_to_label = map_label_to_id(y_train_labels)

    num_files_train = len(x_train_text)
    num_labels_train = len(set(y_train_labels))

    num_files_test = len(x_test_text)
    num_labels_test = len(set(y_test_labels))

    log.info("Number of labels found in training dataset: %d" % (num_labels_train))
    log.info("Number of posts found in training dataset: %d" % (num_files_train))

    log.info("Number of labels found in test dataset: %d" % (num_labels_test))
    log.info("Number of posts found in test dataset: %d" % (num_files_test))

    assert num_labels_train == num_labels_test

    # Break up entire text to words
    all_words = map_text_list_to_word_list(x_train_text)

    vocab, vocabulary_size, top_vocabulary, top_vocabulary_size, reserved_word_size, word_to_id, id_to_word = \
        map_word_list_to_vocabulary(all_words, TOP_VOCABULARY_SIZE)

    # Convert train text to tokens
    x_train_raw, y_train = map_text_to_word_id(x_train_text, y_train_labels,
                                                  top_vocabulary_size, reserved_word_size, num_labels_train,
                                                  label_to_id, word_to_id)

    x_test_raw, y_test = map_text_to_word_id(x_test_text, y_test_labels,
                                                top_vocabulary_size, reserved_word_size, num_labels_test,
                                                label_to_id,
                                                word_to_id)

    x_train = keras.preprocessing.sequence.pad_sequences(x_train_raw,
                                                         value=word_to_id["<PAD>"],
                                                         padding='post',
                                                         truncating='post',
                                                         maxlen=NUM_WORDS_PER_DOC)

    x_test = keras.preprocessing.sequence.pad_sequences(x_test_raw,
                                                        value=word_to_id["<PAD>"],
                                                        padding='post',
                                                        truncating='post',
                                                        maxlen=NUM_WORDS_PER_DOC)

    # Set up a model
    model = Sequential()
    model.add(Embedding(top_vocabulary_size + reserved_word_size, 128))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(20, activation='softmax'))
    # Note the use of tf.train.AdamOptimizer instead of tf.keras.optimizers.Adam
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  optimizer=optimizer, metrics=['accuracy'])

    model.fit(x=x_train, y=y_train, epochs=EPOCH_SIZE)

    # Test against test dataset
    y_hat_test_one_hot = model.predict(x_test)

    total_size = y_hat_test_one_hot.shape[0]
    y_hat_test_one_hot_int = np.argmax(y_hat_test_one_hot, axis=1)  # to int from one-hot vector
    y_test_int = np.argmax(y_test, axis=1)  # to int from one-hot vector

    matched_indices = (y_hat_test_one_hot_int == y_test_int)
    matched_count = y_test[matched_indices].shape[0]
    log.info(
        "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""

    parser = argparse.ArgumentParser(description='Newsgroup post classifier')
    parser.add_argument('--data_dir',
                        type=str,
                        help="Data directory")

    args = parser.parse_args()
    data_dir = args.data_dir

    example(data_dir=data_dir)


if __name__ == "__main__":
    main()
