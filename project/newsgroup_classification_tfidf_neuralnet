#!/usr/bin/env python
"""
Newsgroup post classification

References
----------
https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging
import argparse
import sys
from pathlib import Path
import numpy as np

import tensorflow as tf
import keras

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

from project.load_text_data import load_text_and_label_id_from_files

EPOCH_SIZE = 5

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging


# Example of data dir:
# ../../../../ai/dataset/20/20newsgroups_fromto_only/20news-18828
# Specify this with --data_dir option on the command line

def example(data_dir=None):
    """Train the model and predict.

    Parameters
    ----------
    data_dir: str
        Directory where the data is stored.

    raises
    -------
    ValueError
        If data_dir is not set.
    """
    if data_dir is None:
        raise ValueError("data_dir is not set.")

    # Load text data
    (x_train_text, y_train_id), (x_test_text, y_test_id), (label_to_id, id_to_label) = \
        load_text_and_label_id_from_files(Path(data_dir), test_dataset_ratio=0.2, errors='ignore')

    y_train_oh = keras.utils.to_categorical(y_train_id, 20).astype(np.float32)

    num_files_train = len(x_train_text)
    num_labels_train = len(set(y_train_id))

    num_files_test = len(x_test_text)
    num_labels_test = len(set(y_test_id))

    log.info("Number of labels found in training dataset: %d" % (num_labels_train))
    log.info("Number of posts found in training dataset: %d" % (num_files_train))

    log.info("Number of labels found in test dataset: %d" % (num_labels_test))
    log.info("Number of posts found in test dataset: %d" % (num_files_test))

    vectorizer = Pipeline([
        ('vect', CountVectorizer()),
        ('tfidf', TfidfTransformer())
    ])

    x_train_vectors = vectorizer.fit_transform(x_train_text)
    x_test_vectors = vectorizer.transform(x_test_text)

    # Set up a model
    model = Sequential()
    model.add(Dense(128, activation='relu', input_shape=(x_train_vectors.shape[1],)))
    model.add(Dense(20, activation='softmax'))
    # Note the use of tf.train.AdamOptimizer instead of tf.keras.optimizers.Adam
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  optimizer=optimizer, metrics=['accuracy'])

    model.fit(x=x_train_vectors, y=y_train_oh, epochs=EPOCH_SIZE)

    # Test against test dataset
    y_hat_test_one_hot = model.predict(x_test_vectors)

    total_size = y_hat_test_one_hot.shape[0]
    y_hat_test_id = np.argmax(y_hat_test_one_hot, axis=1)  # to int from one-hot vector

    matched_indices = (y_hat_test_id == y_test_id)
    matched_count = y_test_id[matched_indices].shape[0]
    log.info(
        "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""

    parser = argparse.ArgumentParser(description='Newsgroup post classifier')
    parser.add_argument('--data_dir',
                        type=str,
                        help="Data directory")

    args = parser.parse_args()
    data_dir = args.data_dir

    example(data_dir=data_dir)


if __name__ == "__main__":
    main()
