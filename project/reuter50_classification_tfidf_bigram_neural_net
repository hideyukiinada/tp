#!/usr/bin/env python
"""
Reuter_50_50 Dataset classification using bigrams.

References
----------
https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#
https://scikit-learn.org/stable/datasets/id.html#loading-other-datasets
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging
import argparse
from pathlib import Path
import numpy as np

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

import tensorflow as tf
import keras

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense

from project.load_text_data import load_text_and_label_id_from_training_and_test_dirs

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCH_SIZE = 5

# Example of training dataset dir:
# ../../../../ai/dataset/reuters50/C50train
# ../../../../ai/dataset/reuters50/C50test

# Specify this with --train_data_dir and --test_data_dir option on the command line

def example(train_data_dir=None, test_data_dir=None):
    """Train the model and predict.

    Parameters
    ----------
    train_data_dir: str
        Directory where the training dataset is stored.
    test_data_dir: str
        Directory where the test dataset is stored.

    raises
    -------
    ValueError
        If train_data_dir or test_data_dir is not set.
    """
    if train_data_dir is None:
        raise ValueError("train_data_dir is not set.")

    if test_data_dir is None:
        raise ValueError("test_data_dir is not set.")

    # Load text data
    (x_train_text, y_train_id), (x_test_text, y_test_id), (label_to_id, id_to_label) = \
        load_text_and_label_id_from_training_and_test_dirs(Path(train_data_dir), Path(test_data_dir))

    y_train_oh = keras.utils.to_categorical(y_train_id, 50).astype(np.float32)

    num_files_train = len(x_train_text)
    num_labels_train = len(set(y_train_id))

    num_files_test = len(x_test_text)
    num_labels_test = len(set(y_test_id))

    log.info("Number of labels found in training dataset: %d" % (num_labels_train))
    log.info("Number of posts found in training dataset: %d" % (num_files_train))

    log.info("Number of labels found in test dataset: %d" % (num_labels_test))
    log.info("Number of posts found in test dataset: %d" % (num_files_test))

    tfid = Pipeline([
        ('vect', CountVectorizer(ngram_range=(1, 2))), # select bigrams (min 1 word, max 2 words)
        ('tfidf', TfidfTransformer())
    ])

    x_train_vectors = tfid.fit_transform(x_train_text)  # Convert training dataset to tfid matrix
    x_test_vectors = tfid.transform(x_test_text)  # Convert test set to tfid matrix

    # Set up a model
    model = Sequential()
    model.add(Dense(128, activation='relu', input_shape=(x_train_vectors.shape[1],)))
    model.add(Dense(50, activation='softmax'))
    # Note the use of tf.train.AdamOptimizer instead of tf.keras.optimizers.Adam
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  optimizer=optimizer, metrics=['accuracy'])

    model.fit(x=x_train_vectors, y=y_train_oh, epochs=EPOCH_SIZE)

    # Test against test dataset
    y_hat_test_one_hot = model.predict(x_test_vectors)

    total_size = y_hat_test_one_hot.shape[0]
    y_hat_test_id = np.argmax(y_hat_test_one_hot, axis=1)  # to int from one-hot vector

    matched_indices = (y_hat_test_id == y_test_id)
    matched_count = y_test_id[matched_indices].shape[0]
    log.info(
        "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""

    parser = argparse.ArgumentParser(description='Reuter 50 news author classifier')
    parser.add_argument('--train_data_dir',
                        type=str,
                        default="../../../../ai/dataset/reuters50/C50train",
                        help="Training dataset directory")
    parser.add_argument('--test_data_dir',
                        type=str,
                        default="../../../../ai/dataset/reuters50/C50test",
                        help="Test dataset directory")
    args = parser.parse_args()
    train_data_dir = args.train_data_dir
    test_data_dir = args.test_data_dir

    example(train_data_dir=train_data_dir, test_data_dir=test_data_dir)


if __name__ == "__main__":
    main()
